import argparse
import contextlib
import datetime
import errno
import json
import os
import pathlib
import tempfile
import typing

import anndata
import rich
import numpy
import pandas
import scipy.sparse
from scipy.cluster.hierarchy import fcluster

try:
    import argcomplete
except ImportError as err:
    argcomplete = err

try:
    from rich_argparse import RichHelpFormatter as HelpFormatter
except ImportError:
    from argparse import HelpFormatter

from . import __version__
from .seqio import BaseDataset, GenBankDataset, FastaGFFDataset, DefenseFinderDataset

from .cluster_extractor import GenericClusterAdapter, DefenseFinderAdapter
from .mmseqs import MMSeqs, Database, Clustering
from .hca import manhattan, linkage



def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="igua",
        formatter_class=HelpFormatter,
        add_help=False,
        description=(
            "A method for content-agnostic high-throughput identification of "
            "Gene Cluster Families (GCFs) from gene clusters of genomic and "
            "metagenomic origin. "
        ),
    )
    parser.add_argument(
        "-h",
        "--help",
        help="Display this help message and exit.",
        action="help",
    )
    parser.add_argument(
        "-V",
        "--version",
        help="Display the program version and exit.",
        action="version",
        version=f"igua v{__version__}",
    )
    parser.add_argument(
        "-j",
        "--jobs",
        help="The number of threads to use in parallel sections.",
        type=int,
        default=os.cpu_count(),
        metavar="N",
    )

    group_input = parser.add_argument_group(
        "Input",
        "Input files for the pipeline.",
    )
    group_input.add_argument(
        "-i",
        "--input",
        help="Input files or metadata TSV containing file paths.",
        action="append",
        type=pathlib.Path,
        default=[],
        required=False,
    )
    group_input.add_argument(
        "--dataset-type",
        help="Dataset type: 'genbank' for GenBank files, 'defense-finder' for DefenseFinder output, 'gene-cluster' for generic cluster TSV",
        choices=["genbank", "defense-finder", "gene-cluster"],
        default=None,
    )

    group_output = parser.add_argument_group(
        "Output", "Output files generated by the pipeline."
    )
    group_output.add_argument(
        "-o",
        "--output",
        help="The name of the output file to generate.",
        default=pathlib.Path("gcfs.tsv"),
        type=pathlib.Path,
    )
    group_output.add_argument(
        "-C",
        "--compositions",
        help="A file where to write compositional data for GCF representatives.",
        type=pathlib.Path,
    )
    group_output.add_argument(
        "-F",
        "--features",
        help="A file where to write protein cluster representatives in FASTA format.",
        type=pathlib.Path,
    )

    group_parameters = parser.add_argument_group(
        "Parameters", "General purpose parameters."
    )
    group_parameters.add_argument(
        "-w",
        "--workdir",
        help="A folder to use for temporary data produced by MMSeqs2.",
    )
    group_parameters.add_argument(
        "--prefix",
        help="The prefix for GCF identifiers generated by the pipeline.",
        default="GCF",
    )

    group_clustering = parser.add_argument_group(
        "Clustering", "Parameters to control the hierarchical clustering."
    )
    group_clustering.add_argument(
        "--no-clustering",
        help="Disable the protein-level clustering.",
        action="store_false",
        dest="clustering",
    )
    group_clustering.add_argument(
        "--clustering-method",
        help="The hierarchical method to use for protein-level clustering.",
        default="average",
        choices={
            "average",
            "single",
            "complete",
            "weighted",
            "centroid",
            "median",
            "ward",
        },
    )
    group_clustering.add_argument(
        "--clustering-distance",
        help="The distance threshold after which to stop merging clusters.",
        type=float,
        default=0.8,
    )
    group_clustering.add_argument(
        "--precision",
        help="The numerical precision to use for computing distances for hierarchical clustering.",
        default="double",
        choices={"half", "single", "double"},
    )

    group_mmseqs_dedup = parser.add_argument_group(
        "MMSeqs2 Deduplication",
        "Parameters for the first nucleotide clustering step (exact/near-exact deduplication).",
    )
    group_mmseqs_dedup.add_argument(
        "--dedup-identity",
        help="Sequence identity threshold for deduplication step.",
        type=float,
        default=0.85,
        metavar="FLOAT",
    )
    group_mmseqs_dedup.add_argument(
        "--dedup-coverage",
        help="Coverage threshold for deduplication step.",
        type=float,
        default=1.0,
        metavar="FLOAT",
    )
    group_mmseqs_dedup.add_argument(
        "--dedup-evalue",
        help="E-value threshold for deduplication step.",
        type=float,
        default=0.001,
        metavar="FLOAT",
    )
    group_mmseqs_dedup.add_argument(
        "--dedup-cluster-mode",
        help="Clustering mode for deduplication: 0=SetCover, 1=Connected component, 2=Greedy incremental.",
        type=int,
        default=0,
        choices=[0, 1, 2],
        metavar="INT",
    )
    group_mmseqs_dedup.add_argument(
        "--dedup-coverage-mode",
        help="Coverage mode for deduplication: 0=target, 1=query, 2=both, 3=length of target, 4=length of query, 5=length of both.",
        type=int,
        default=1,
        choices=[0, 1, 2, 3, 4, 5],
        metavar="INT",
    )
    group_mmseqs_dedup.add_argument(
        "--dedup-spaced-kmer-mode",
        help="Spaced k-mer mode for deduplication: 0=use ungapped k-mers, 1=use spaced k-mers.",
        type=int,
        default=0,
        choices=[0, 1],
        metavar="INT",
    )

    group_mmseqs_nuc = parser.add_argument_group(
        "MMSeqs2 Nucleotide Clustering",
        "Parameters for the second nucleotide clustering step (relaxed clustering of representatives).",
    )
    group_mmseqs_nuc.add_argument(
        "--nuc-identity",
        help="Sequence identity threshold for nucleotide clustering step.",
        type=float,
        default=0.6,
        metavar="FLOAT",
    )
    group_mmseqs_nuc.add_argument(
        "--nuc-coverage",
        help="Coverage threshold for nucleotide clustering step.",
        type=float,
        default=0.5,
        metavar="FLOAT",
    )
    group_mmseqs_nuc.add_argument(
        "--nuc-evalue",
        help="E-value threshold for nucleotide clustering step.",
        type=float,
        default=0.001,
        metavar="FLOAT",
    )
    group_mmseqs_nuc.add_argument(
        "--nuc-cluster-mode",
        help="Clustering mode for nucleotide step: 0=SetCover, 1=Connected component, 2=Greedy incremental.",
        type=int,
        default=0,
        choices=[0, 1, 2],
        metavar="INT",
    )
    group_mmseqs_nuc.add_argument(
        "--nuc-coverage-mode",
        help="Coverage mode for nucleotide step: 0=target, 1=query, 2=both, 3=length of target, 4=length of query, 5=length of both.",
        type=int,
        default=0,
        choices=[0, 1, 2, 3, 4, 5],
        metavar="INT",
    )
    group_mmseqs_nuc.add_argument(
        "--nuc-spaced-kmer-mode",
        help="Spaced k-mer mode for nucleotide step: 0=use ungapped k-mers, 1=use spaced k-mers.",
        type=int,
        default=0,
        choices=[0, 1],
        metavar="INT",
    )

    group_mmseqs_prot = parser.add_argument_group(
        "MMSeqs2 Protein Clustering",
        "Parameters for protein clustering step (used for compositional analysis).",
    )
    group_mmseqs_prot.add_argument(
        "--prot-identity",
        help="Sequence identity threshold for protein clustering step.",
        type=float,
        default=0.5,
        metavar="FLOAT",
    )
    group_mmseqs_prot.add_argument(
        "--prot-coverage",
        help="Coverage threshold for protein clustering step.",
        type=float,
        default=0.9,
        metavar="FLOAT",
    )
    group_mmseqs_prot.add_argument(
        "--prot-evalue",
        help="E-value threshold for protein clustering step.",
        type=float,
        default=0.001,
        metavar="FLOAT",
    )
    group_mmseqs_prot.add_argument(
        "--prot-coverage-mode",
        help="Coverage mode for protein step: 0=target, 1=query, 2=both, 3=length of target, 4=length of query, 5=length of both.",
        type=int,
        default=1,
        choices=[0, 1, 2, 3, 4, 5],
        metavar="INT",
    )

    group_dataset = parser.add_argument_group(
        "Dataset Configuration",
        "Configuration for dataset-specific options.",
    )
    group_dataset.add_argument(
        "--activity",
        help="Filter by defense system activity (defense-finder only): 'all' (default), 'defense', 'antidefense'",
        default="all",
        choices={"defense", "antidefense", "all"},
    )
    group_dataset.add_argument(
        "--column-mapping",
        help="JSON file mapping column names for gene-cluster format (e.g., '{\"sys_id\":\"cluster_id\",...}')",
        type=pathlib.Path,
    )
    group_dataset.add_argument(
        "--verbose",
        help="Detailed output for extracted cluster sequences",
        action="store_true",
        default=False,
    )
    group_dataset.add_argument(
        "--systems-tsv",
        help="Path to systems/clusters TSV file (requires --genes-tsv, --gff-file, --genome-fasta, --protein-fasta)",
        type=pathlib.Path,
    )
    group_dataset.add_argument(
        "--genes-tsv",
        help="Path to cluster genes TSV file",
        type=pathlib.Path,
    )
    group_dataset.add_argument(
        "--gff-file",
        help="Path to GFF annotation file",
        type=pathlib.Path,
    )
    group_dataset.add_argument(
        "--genome-fasta",
        help="Path to genome FASTA file",
        type=pathlib.Path,
    )
    group_dataset.add_argument(
        "--protein-fasta",
        help="Path to protein FASTA file (.faa)",
        type=pathlib.Path,
    )

    return parser



def create_dataset(
    progress: rich.progress.Progress,
    args: argparse.Namespace,
) -> BaseDataset:
    """Create dataset handler with appropriate adapter."""
    
    dataset_type = args.dataset_type
    
    if dataset_type is None:
        if args.input:
            first_file = args.input[0]
            if first_file.suffix.lower() in {".gb", ".gbk", ".genbank"}:
                dataset_type = "genbank"
            elif first_file.suffix.lower() == ".tsv":
                dataset_type = "gene-cluster"
        else:
            dataset_type = "gene-cluster"
    
    if dataset_type == "genbank":
        progress.console.print(f"[bold blue]{'Mode':>12}[/] GenBank files")
        return GenBankDataset(inputs=args.input)
    
    elif dataset_type == "defense-finder":
        adapter = DefenseFinderAdapter(activity_filter=args.activity)
        
        if args.column_mapping:
            with open(args.column_mapping) as f:
                column_mapping = json.load(f)
            adapter._column_mapping.update(column_mapping)
            progress.console.print(
                f"[bold blue]{'Mapping':>12}[/] [magenta]{args.column_mapping}[/]"
            )
        
        dataset = DefenseFinderDataset(inputs=args.input, adapter=adapter) 
        dataset.verbose = args.verbose
        progress.console.print(f"[bold blue]{'Mode':>12}[/] DefenseFinder format")
        if args.input:
            progress.console.print(f"[bold blue]{'Metadata':>12}[/] [magenta]{args.input[0]}[/]")
        
        return dataset
    
    else: 
        column_mapping = None
        if args.column_mapping:
            with open(args.column_mapping) as f:
                column_mapping = json.load(f)
            progress.console.print(
                f"[bold blue]{'Mapping':>12}[/] [magenta]{args.column_mapping}[/]"
            )
        
        adapter = GenericClusterAdapter(column_mapping=column_mapping)
        dataset = FastaGFFDataset(inputs=args.input, adapter=adapter)
        dataset.verbose = args.verbose
        
        if args.input:
            progress.console.print(f"[bold blue]{'Mode':>12}[/] Generic gene cluster format")
            progress.console.print(f"[bold blue]{'Metadata':>12}[/] [magenta]{args.input[0]}[/]")
        
        if not args.column_mapping:
            progress.console.print(
                f"[bold yellow]{'Note':>12}[/] Using default column names: sys_id, sys_beg, sys_end, protein_in_syst"
            )
        
        return dataset


def get_mmseqs_params(args: argparse.Namespace) -> typing.Tuple[dict, dict, dict]:
    """Build MMSeqs2 parameter dictionaries from command-line arguments."""

    params_nuc1 = dict(
        e_value=args.dedup_evalue,
        sequence_identity=args.dedup_identity,
        coverage=args.dedup_coverage,
        cluster_mode=args.dedup_cluster_mode,
        coverage_mode=args.dedup_coverage_mode,
        spaced_kmer_mode=args.dedup_spaced_kmer_mode,
    )

    params_nuc2 = dict(
        e_value=args.nuc_evalue,
        sequence_identity=args.nuc_identity,
        coverage=args.nuc_coverage,
        cluster_mode=args.nuc_cluster_mode,
        coverage_mode=args.nuc_coverage_mode,
        spaced_kmer_mode=args.nuc_spaced_kmer_mode,
    )

    params_prot = dict(
        e_value=args.prot_evalue,
        coverage=args.prot_coverage,
        coverage_mode=args.prot_coverage_mode,
        sequence_identity=args.prot_identity,
    )

    return params_nuc1, params_nuc2, params_prot


def get_protein_representative(
    mmseqs: MMSeqs,
    input_path: pathlib.Path,
    output_prefix: pathlib.Path,
    fasta_path: pathlib.Path,
) -> None:
    db = Database(mmseqs, input_path.with_suffix(".db"))
    result = Clustering(mmseqs, output_prefix.with_suffix(".db"), db)
    subdb = result.to_subdb(output_prefix.with_name(f"{output_prefix.name}_rep_seq.db"))
    subdb.to_fasta(fasta_path)


def make_compositions(
    progress: rich.progress.Progress,
    protein_clusters: pandas.DataFrame,
    representatives: typing.Dict[str, int],
    protein_representatives: typing.Dict[str, int],
    protein_sizes: typing.Dict[str, int],
) -> anndata.AnnData:
    compositions = scipy.sparse.dok_matrix(
        (len(representatives), len(protein_representatives)), dtype=numpy.int32
    )

    task = progress.add_task(
        description=f"[bold blue]{'Working':>9}[/]", total=len(protein_clusters)
    )
    for row in progress.track(protein_clusters.itertuples(), task_id=task):
        cluster_index = representatives[row.cluster_id]
        prot_index = protein_representatives[row.protein_representative]
        compositions[cluster_index, prot_index] += protein_sizes[
            row.protein_representative
        ]
    progress.remove_task(task)

    sorted_representatives = sorted(representatives, key=representatives.__getitem__)
    sorted_protein_representatives = sorted(
        protein_representatives, key=protein_representatives.__getitem__
    )
    return anndata.AnnData(
        X=compositions.tocsr(),
        obs=pandas.DataFrame(
            index=pandas.Index(sorted_representatives, name="cluster_id")
        ),
        var=pandas.DataFrame(
            index=pandas.Index(sorted_protein_representatives, name="protein_id"),
            data=dict(size=[protein_sizes[x] for x in sorted_protein_representatives]),
        ),
    )


def compute_distances(
    progress: rich.progress.Progress,
    compositions: scipy.sparse.csr_matrix,
    jobs: typing.Optional[int],
    precision: str,
) -> numpy.ndarray:
    n = 0
    r = compositions.shape[0]
    # compute the number of amino acids per cluster
    clusters_aa = numpy.zeros(r, dtype=numpy.int32)
    clusters_aa[:] = compositions.sum(axis=1).A1
    # make sure the sparse matrix has sorted indices (necessary for
    # the distance algorithm to work efficiently)
    if not compositions.has_sorted_indices:
        compositions.sort_indices()
    # compute manhattan distance on sparse matrix
    distance_vector = numpy.zeros(r * (r - 1) // 2, dtype=precision)
    manhattan(
        compositions.data,
        compositions.indices,
        compositions.indptr,
        distance_vector,
        threads=jobs,
    )
    # ponderate by sum of amino-acid distance
    for i in range(r - 1):
        l = r - (i + 1)
        distance_vector[n : n + l] /= (clusters_aa[i + 1 :] + clusters_aa[i]).clip(
            min=1
        )
        n += l
    # check distances are in [0, 1]
    return numpy.clip(distance_vector, 0.0, 1.0, out=distance_vector)


def main(argv: typing.Optional[typing.List[str]] = None) -> int:
    # build parser and get arguments
    parser = build_parser()
    if not isinstance(argcomplete, ImportError):
        argcomplete.autocomplete(parser)
    args = parser.parse_args(argv)

    start_time = datetime.datetime.now()
    start_time_str = start_time.strftime("%Y-%m-%d %H:%M:%S")

    params_nuc1, params_nuc2, params_prot = get_mmseqs_params(args)

    if args.workdir is None:
        workdir = pathlib.Path(tempfile.mkdtemp())
    else:
        workdir = pathlib.Path(args.workdir)
        workdir.mkdir(parents=True, exist_ok=True)

    # validate individual file arguments
    individual_args = [
        args.systems_tsv,
        args.genes_tsv,
        args.gff_file,
        args.genome_fasta,
        args.protein_fasta,
    ]

    using_individual_files = any(individual_args)

    if using_individual_files:
        if not all(individual_args):
            parser.error(
                "Individual file mode requires ALL of: "
                "--systems-tsv, --genes-tsv, --gff-file, --genome-fasta, --protein-fasta"
            )

        if not args.input:
            input_dict_df = {
                "genome_id": [os.path.basename(args.genome_fasta).split(".")[0]],
                "systems_tsv": [str(args.systems_tsv)],
                "genes_tsv": [str(args.genes_tsv)],
                "gff_file": [str(args.gff_file)],
                "genome_fasta_file": [str(args.genome_fasta)],
                "protein_fasta_file": [str(args.protein_fasta)],
            }

            input_df = pandas.DataFrame(input_dict_df)
            temp_tsv = workdir / "metadata.tsv"
            input_df.to_csv(temp_tsv, sep="\t", index=False)
            args.input = [temp_tsv]
    elif not args.input:
        parser.error("Input files (-i/--input) are required")

    with contextlib.ExitStack() as ctx:
        # prepare progress bar
        progress = ctx.enter_context(
            rich.progress.Progress(
                "",
                rich.progress.SpinnerColumn(),
                *rich.progress.Progress.get_default_columns(),
            )
        )

        progress.console.print(f"[bold blue]{'Started':>12}[/] {start_time_str}")

        mmseqs = MMSeqs(progress=progress, threads=args.jobs, tempdir=workdir)

        # check mmseqs version
        try:
            v = mmseqs.version()
            progress.console.print(f"[bold green]{'Using':>12}[/] MMseqs2 {v!r}")
        except RuntimeError:
            progress.console.print(
                f"[bold red]{'Failed':>12}[/] to locate MMseqs2 binary"
            )
            return errno.ENOENT

        # create appropriate dataset handler
        dataset = create_dataset(progress=progress, args=args)

        # extract raw sequences
        clusters_fna = workdir.joinpath("clusters.fna")
        progress.console.print(f"[bold blue]{'Loading':>12}[/] input clusters")
        input_sequences = dataset.extract_sequences(progress, clusters_fna)
        progress.console.print(
            f"[bold green]{'Loaded':>12}[/] {len(input_sequences)} clusters to process"
        )

        # create initial sequence database
        progress.console.print(
            f"[bold blue]{'Starting':>12}[/] nucleotide deduplication step with [purple]mmseqs[/]"
        )
        db = Database.create(mmseqs, clusters_fna)
        step1 = db.cluster(workdir / "step1.db", **params_nuc1)
        gcfs1 = step1.to_dataframe(columns=["fragment_representative", "cluster_id"]).sort_values("cluster_id")  # type: ignore
        progress.console.print(
            f"[bold green]{'Reduced':>12}[/] {len(gcfs1)} clusters to {len(gcfs1.fragment_representative.unique())} complete representatives"
        )

        # cluster sequences
        progress.console.print(
            f"[bold blue]{'Starting':>12}[/] nucleotide clustering step with [purple]MMSeqs2[/]"
        )
        repdb = step1.to_subdb(workdir / "step1.rep_seq.db")
        step2 = repdb.cluster(workdir / "step2.db", **params_nuc2)
        gcfs2 = step2.to_dataframe(columns=["nucleotide_representative", "fragment_representative"]).sort_values("fragment_representative")  # type: ignore
        progress.console.print(
            f"[bold green]{'Reduced':>12}[/] {len(gcfs2)} clusters to {len(gcfs2.nucleotide_representative.unique())} nucleotide representatives"
        )

        # load representatives
        progress.console.print(
            f"[bold blue]{'Extracting':>12}[/] representative clusters"
        )
        representatives = {
            x: i
            for i, x in enumerate(sorted(gcfs2["nucleotide_representative"].unique()))
        }
        progress.console.print(
            f"[bold green]{'Found':>12}[/] {len(representatives)} nucleotide representative clusters"
        )

        if args.clustering and len(representatives) > 1:
            # extract proteins and record sizes
            proteins_faa = workdir.joinpath("proteins.faa")
            progress.console.print(
                f"[bold blue]{'Extracting':>12}[/] protein sequences from representative clusters"
            )

            protein_sizes = dataset.extract_proteins(
                progress, args.input, proteins_faa, representatives
            )

            if not proteins_faa.exists() or proteins_faa.stat().st_size == 0:
                progress.console.print(
                    f"[bold yellow]{'Warning':>12}[/] No proteins extracted from defense systems"
                )
                progress.console.print(
                    f"[bold yellow]{'Skipping':>12}[/] protein clustering due to empty protein file"
                )
                args.clustering = False

            # cluster proteins
            prot_db = Database.create(mmseqs, proteins_faa)
            prot_result = prot_db.cluster(workdir / "step3.db", **params_prot)
            prot_clusters = prot_result.to_dataframe(
                columns=["protein_representative", "protein_id"]
            )

            # extract protein representatives
            if isinstance(dataset, DefenseFinderDataset):
                # double underscore for DefenseFinder
                prot_clusters["cluster_id"] = (
                    prot_clusters["protein_id"].str.rsplit("__", n=1).str[0]
                )
            else:
                # traditional format: use single underscore delimiter
                prot_clusters["cluster_id"] = (
                    prot_clusters["protein_id"].str.rsplit("_", n=1).str[0]
                )

            protein_representatives = {
                x: i
                for i, x in enumerate(
                    sorted(prot_clusters["protein_representative"].unique())
                )
            }
            progress.console.print(
                f"[bold green]{'Found':>12}[/] {len(protein_representatives)} protein representatives for {len(prot_clusters)} proteins"
            )

            # build weighted compositional array
            progress.console.print(
                f"[bold blue]{'Building':>12}[/] weighted compositional array"
            )
            compositions = make_compositions(
                progress,
                prot_clusters,
                representatives,
                protein_representatives,
                protein_sizes,
            )

            # compute and ponderate distances
            progress.console.print(
                f"[bold blue]{'Computing':>12}[/] pairwise distance based on protein composition"
            )
            distance_vector = compute_distances(
                progress, compositions.X, args.jobs, args.precision
            )

            # run hierarchical clustering
            progress.console.print(
                f"[bold blue]{'Clustering':>12}[/] gene clusters using {args.clustering_method} linkage"
            )
            Z = linkage(distance_vector, method=args.clustering_method)
            flat = fcluster(Z, criterion="distance", t=args.clustering_distance)

            # build GCFs based on flat clustering
            gcfs3 = pandas.DataFrame(
                {
                    "gcf_id": [f"{args.prefix}{i:07}" for i in flat],
                    "nucleotide_representative": compositions.obs_names,
                }
            )
        else:
            sorted_representatives = sorted(
                representatives, key=representatives.__getitem__
            )
            gcfs3 = pandas.DataFrame(
                {
                    "gcf_id": [
                        f"{args.prefix}{i+1:07}"
                        for i in range(len(sorted_representatives))
                    ],
                    "nucleotide_representative": sorted_representatives,
                }
            )

        progress.console.print(
            f"[bold green]{'Built':>12}[/] {len(gcfs3.gcf_id.unique())} GCFs from {len(input_sequences)} initial clusters"
        )

        # build GCFs based on flat clustering
        gcf3_representatives = (
            pandas.merge(
                gcfs3,
                input_sequences["cluster_length"],
                left_on="nucleotide_representative",
                right_index=True,
            )
            .sort_values("cluster_length")
            .drop_duplicates("gcf_id", keep="last")
            .set_index("gcf_id")
        )
        gcfs3 = pandas.merge(
            gcfs3,
            gcf3_representatives["nucleotide_representative"].rename(
                "gcf_representative"
            ),
            left_on="gcf_id",
            right_index=True,
        )

        # build final GCF table
        gcfs = pandas.merge(
            pandas.merge(
                pandas.merge(gcfs1, gcfs2, on="fragment_representative"),
                gcfs3,
                on="nucleotide_representative",
            ),
            input_sequences,
            left_on="cluster_id",
            right_index=True,
        )

        # save results
        gcfs.sort_values(["gcf_id", "cluster_length"], inplace=True)
        gcfs = gcfs[
            [
                "cluster_id",
                "cluster_length",
                "gcf_id",
                "gcf_representative",
                "nucleotide_representative",
                "fragment_representative",
                "filename",
            ]
        ]
        gcfs.to_csv(args.output, sep="\t", index=False)
        progress.console.print(
            f"[bold green]{'Saved':>12}[/] final GCFs table to {str(args.output)!r}"
        )

        # save compositions
        if args.compositions is not None:
            gcf_representatives = gcfs["gcf_representative"].unique()
            representatives_compositions = anndata.AnnData(
                X=compositions[gcf_representatives].X,
                var=compositions.var,
                obs=(
                    gcfs.set_index("cluster_id")
                    .loc[gcf_representatives, ["gcf_id", "gcf_representative"]]
                    .set_index("gcf_id")
                ),
            )
            representatives_compositions.write(args.compositions)
            progress.console.print(
                f"[bold green]{'Saved':>12}[/] GCF compositions to {str(args.compositions)!r}"
            )

        # save protein features
        if args.features is not None:
            get_protein_representative(
                mmseqs,
                proteins_faa,
                workdir.joinpath("step3"),
                args.features,
            )
            progress.console.print(
                f"[bold green]{'Saved':>12}[/] protein features to {str(args.features)!r}"
            )

    end_time = datetime.datetime.now()
    end_time_str = end_time.strftime("%Y-%m-%d %H:%M:%S")
    total_time = end_time - start_time

    total_seconds = int(total_time.total_seconds())
    hours, remainder = divmod(total_seconds, 3600)
    minutes, seconds = divmod(remainder, 60)

    if hours > 0:
        time_str = f"{hours}h {minutes}m {seconds}s"
    elif minutes > 0:
        time_str = f"{minutes}m {seconds}s"
    else:
        time_str = f"{seconds}s"

    progress.console.print(f"[bold blue]{'Completed':>12}[/] {end_time_str}")
    progress.console.print(f"[bold green]{'Total time':>12}[/] {time_str}")

    return 0