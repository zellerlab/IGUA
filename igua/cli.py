import argparse
import contextlib
import datetime
import errno
import json
import os
import pathlib
import tempfile
import typing
import sys

import anndata
import rich
import numpy
import pandas
import scipy.sparse
from scipy.cluster.hierarchy import fcluster

try:
    import argcomplete
except ImportError as err:
    argcomplete = err

try:
    from rich_argparse import RichHelpFormatter as HelpFormatter
except ImportError:
    from argparse import HelpFormatter

from . import __version__
from .dataset.base import BaseDataset
from .dataset.genbank import GenBankDataset
from .dataset.fasta_gff import FastaGFFDataset
from .dataset.defensefinder import DefenseFinderDataset
from .mmseqs import MMSeqs, Database, Clustering
from .pipeline import ClusteringParameters, ClusteringPipeline


def build_parser() -> argparse.ArgumentParser:
    show_all = '--help-all' in sys.argv

    if show_all:
        sys.argv = [arg if arg != '--help-all' else '--help' for arg in sys.argv]

    def help_text(text: str) -> str:
        return text if show_all else argparse.SUPPRESS

    parser = argparse.ArgumentParser(
        prog="igua",
        formatter_class=HelpFormatter,
        add_help=False,
        description=(
            "A method for content-agnostic high-throughput identification of "
            "Gene Cluster Families (GCFs) from gene clusters of genomic and "
            "metagenomic origin. "
        ),
        epilog="Use --help-all to see all available options including advanced parameters." if not show_all else None,
    )
    parser.add_argument(
        "-h",
        "--help",
        help="Display this help message and exit.",
        action="help",
    )

    if not show_all:
        parser.add_argument(
            "--help-all",
            help="Display all options including advanced parameters.",
            action="help",
        )

    parser.add_argument(
        "-V",
        "--version",
        help="Display the program version and exit.",
        action="version",
        version=f"igua v{__version__}",
    )
    parser.add_argument(
        "-j",
        "--jobs",
        help="The number of threads to use in parallel sections.",
        type=int,
        default=os.cpu_count(),
        metavar="N",
    )

    group_input = parser.add_argument_group(
        "Input",
        "Mandatory input files required by the pipeline."
    )
    group_input.add_argument(
        "-i",
        "--input",
        help="Input files or metadata TSV containing file paths.",
        action="append",
        type=pathlib.Path,
        default=[],
        required=True,
    )
    group_input.add_argument(
        "--dataset-type",
        help="Dataset type: 'genbank' for GenBank files, 'manual' for generic cluster TSV, 'defense-finder' for DefenseFinder output.",
        choices=["genbank", "manual", "defense-finder"],
        default=None,
    )

    group_output = parser.add_argument_group(
        "Output", "Output files generated by the pipeline."
    )
    group_output.add_argument(
        "-o",
        "--output",
        help="The name of the output file to generate.",
        default=pathlib.Path("gcfs.tsv"),
        type=pathlib.Path,
    )
    group_output.add_argument(
        "-C",
        "--compositions",
        help=help_text("A file where to write compositional data for GCF representatives."),
        type=pathlib.Path,
    )
    group_output.add_argument(
        "-F",
        "--features",
        help=help_text("A file where to write protein cluster representatives in FASTA format."),
        type=pathlib.Path,
    )

    group_parameters = parser.add_argument_group(
        "Parameters", "General purpose parameters."
    )
    group_parameters.add_argument(
        "-w",
        "--workdir",
        help=help_text("A folder to use for temporary data produced by MMSeqs2."),
    )
    group_parameters.add_argument(
        "--prefix",
        help="The prefix for GCF identifiers generated by the pipeline.",
        default="GCF",
    )

    group_clustering = parser.add_argument_group(
        "Clustering", "Parameters to control the hierarchical clustering."
    )
    group_clustering.add_argument(
        "--no-clustering",
        help=help_text("Disable the protein-level clustering."),
        action="store_false",
        dest="clustering",
    )
    group_clustering.add_argument(
        "--clustering-method",
        help=help_text("The hierarchical method to use for protein-level clustering."),
        default="average",
        choices={
            "average",
            "single",
            "complete",
            "weighted",
            "centroid",
            "median",
            "ward",
        },
    )
    group_clustering.add_argument(
        "--clustering-distance",
        help=help_text("The distance threshold after which to stop merging clusters."),
        type=float,
        default=0.8,
    )
    group_clustering.add_argument(
        "--precision",
        help=help_text("The numerical precision to use for computing distances for hierarchical clustering."),
        default="double",
        choices={"half", "single", "double"},
    )

    group_mmseqs_dedup = parser.add_argument_group(
        "MMSeqs2 Deduplication",
        "Parameters for the first nucleotide clustering step (exact/near-exact deduplication).",
    )
    group_mmseqs_dedup.add_argument(
        "--dedup-identity",
        help=help_text("Sequence identity threshold for deduplication step."),
        type=float,
        default=0.85,
        metavar="FLOAT",
    )
    group_mmseqs_dedup.add_argument(
        "--dedup-coverage",
        help=help_text("Coverage threshold for deduplication step."),
        type=float,
        default=1.0,
        metavar="FLOAT",
    )
    group_mmseqs_dedup.add_argument(
        "--dedup-evalue",
        help=help_text("E-value threshold for deduplication step."),
        type=float,
        default=0.001,
        metavar="FLOAT",
    )
    group_mmseqs_dedup.add_argument(
        "--dedup-cluster-mode",
        help=help_text("Clustering mode for deduplication: 0=SetCover, 1=Connected component, 2=Greedy incremental."),
        type=int,
        default=0,
        choices=[0, 1, 2],
        metavar="INT",
    )
    group_mmseqs_dedup.add_argument(
        "--dedup-coverage-mode",
        help=help_text("Coverage mode for deduplication: 0=target, 1=query, 2=both, 3=length of target, 4=length of query, 5=length of both."),
        type=int,
        default=1,
        choices=[0, 1, 2, 3, 4, 5],
        metavar="INT",
    )
    group_mmseqs_dedup.add_argument(
        "--dedup-spaced-kmer-mode",
        help=help_text("Spaced k-mer mode for deduplication: 0=use ungapped k-mers, 1=use spaced k-mers."),
        type=int,
        default=0,
        choices=[0, 1],
        metavar="INT",
    )

    group_mmseqs_nuc = parser.add_argument_group(
        "MMSeqs2 Nucleotide Clustering",
        "Parameters for the second nucleotide clustering step (relaxed clustering of representatives).",
    )
    group_mmseqs_nuc.add_argument(
        "--nuc-identity",
        help=help_text("Sequence identity threshold for nucleotide clustering step."),
        type=float,
        default=0.6,
        metavar="FLOAT",
    )
    group_mmseqs_nuc.add_argument(
        "--nuc-coverage",
        help=help_text("Coverage threshold for nucleotide clustering step."),
        type=float,
        default=0.5,
        metavar="FLOAT",
    )
    group_mmseqs_nuc.add_argument(
        "--nuc-evalue",
        help=help_text("E-value threshold for nucleotide clustering step."),
        type=float,
        default=0.001,
        metavar="FLOAT",
    )
    group_mmseqs_nuc.add_argument(
        "--nuc-cluster-mode",
        help=help_text("Clustering mode for nucleotide step: 0=SetCover, 1=Connected component, 2=Greedy incremental."),
        type=int,
        default=0,
        choices=[0, 1, 2],
        metavar="INT",
    )
    group_mmseqs_nuc.add_argument(
        "--nuc-coverage-mode",
        help=help_text("Coverage mode for nucleotide step: 0=target, 1=query, 2=both, 3=length of target, 4=length of query, 5=length of both."),
        type=int,
        default=0,
        choices=[0, 1, 2, 3, 4, 5],
        metavar="INT",
    )
    group_mmseqs_nuc.add_argument(
        "--nuc-spaced-kmer-mode",
        help=help_text("Spaced k-mer mode for nucleotide step: 0=use ungapped k-mers, 1=use spaced k-mers."),
        type=int,
        default=0,
        choices=[0, 1],
        metavar="INT",
    )

    group_mmseqs_prot = parser.add_argument_group(
        "MMSeqs2 Protein Clustering",
        "Parameters for protein clustering step (used for compositional analysis).",
    )
    group_mmseqs_prot.add_argument(
        "--prot-identity",
        help=help_text("Sequence identity threshold for protein clustering step."),
        type=float,
        default=0.5,
        metavar="FLOAT",
    )
    group_mmseqs_prot.add_argument(
        "--prot-coverage",
        help=help_text("Coverage threshold for protein clustering step."),
        type=float,
        default=0.9,
        metavar="FLOAT",
    )
    group_mmseqs_prot.add_argument(
        "--prot-evalue",
        help=help_text("E-value threshold for protein clustering step."),
        type=float,
        default=0.001,
        metavar="FLOAT",
    )
    group_mmseqs_prot.add_argument(
        "--prot-coverage-mode",
        help=help_text("Coverage mode for protein step: 0=target, 1=query, 2=both, 3=length of target, 4=length of query, 5=length of both."),
        type=int,
        default=1,
        choices=[0, 1, 2, 3, 4, 5],
        metavar="INT",
    )

    group_dataset = parser.add_argument_group(
        "Dataset Configuration",
        "Configuration for dataset-specific options.",
    )
    group_dataset.add_argument(
        "--activity",
        help=help_text("Filter by defense system activity (defense-finder only): 'all' (default), 'defense', 'antidefense'"),
        default="all",
        choices={"defense", "antidefense", "all"},
    )
    group_dataset.add_argument(
        "--column-mapping",
        help=help_text("JSON file mapping column names for manual format (e.g., '{\"cluster_id\":\"sys_id\",...}')"),
        type=pathlib.Path,
    )
    group_dataset.add_argument(
        "--verbose",
        help=help_text("Detailed output for extracted cluster sequences"),
        action="store_true",
        default=False,
    )
    group_dataset.add_argument(
        "--cluster-tsv",
        help=help_text("Path to clusters TSV file (requires --gff-file, --genome-fasta, --protein-fasta)"),
        type=pathlib.Path,
    )
    group_dataset.add_argument(
        "--gff-file",
        help=help_text("Path to GFF annotation file"),
        type=pathlib.Path,
    )
    group_dataset.add_argument(
        "--genome-fasta",
        help=help_text("Path to genome FASTA file"),
        type=pathlib.Path,
    )
    group_dataset.add_argument(
        "--protein-fasta",
        help=help_text("Path to protein FASTA file (.faa)"),
        type=pathlib.Path,
    )

    return parser


def create_dataset(
    progress: rich.progress.Progress,
    args: argparse.Namespace,
) -> BaseDataset:
    """Create dataset handler with appropriate adapter."""

    dataset_type = args.dataset_type

    if dataset_type is None:
        if args.input:
            first_file = args.input[0]
            if first_file.suffix.lower() in {".gb", ".gbk", ".genbank"}:
                dataset_type = "genbank"
            elif first_file.suffix.lower() == ".tsv":
                dataset_type = "manual"
        else:
            dataset_type = "unspecified"

    if dataset_type == "genbank":
        progress.console.print(
            f"[bold blue]{'Dataset':>12}[/] [magenta]GenBank[/] format"
        )
        return GenBankDataset(args.input)

    elif dataset_type == "manual":
        progress.console.print(
            f"[bold blue]{'Dataset':>12}[/] [magenta]Manual[/] gene cluster format"
        )
        column_mapping = None
        if args.column_mapping:
            with open(args.column_mapping) as f:
                column_mapping = json.load(f)
        return FastaGFFDataset(args.input, column_mapping=column_mapping)

    elif dataset_type == "defense-finder":
        progress.console.print(
            f"[bold blue]{'Dataset':>12}[/] [magenta]DefenseFinder[/] format"
        )
        return DefenseFinderDataset(args.input, activity_filter=args.activity)

    else:
        raise ValueError(f"Unknown dataset type: {dataset_type}")


def get_mmseqs_params(args: argparse.Namespace) -> ClusteringParameters:
    """Build MMSeqs2 parameter dictionaries from command-line arguments."""

    params_nuc1 = dict(
        e_value=args.dedup_evalue,
        sequence_identity=args.dedup_identity,
        coverage=args.dedup_coverage,
        cluster_mode=args.dedup_cluster_mode,
        coverage_mode=args.dedup_coverage_mode,
        spaced_kmer_mode=args.dedup_spaced_kmer_mode,
    )

    params_nuc2 = dict(
        e_value=args.nuc_evalue,
        sequence_identity=args.nuc_identity,
        coverage=args.nuc_coverage,
        cluster_mode=args.nuc_cluster_mode,
        coverage_mode=args.nuc_coverage_mode,
        spaced_kmer_mode=args.nuc_spaced_kmer_mode,
    )

    params_prot = dict(
        e_value=args.prot_evalue,
        coverage=args.prot_coverage,
        coverage_mode=args.prot_coverage_mode,
        sequence_identity=args.prot_identity,
    )

    return ClusteringParameters(
        nuc1=params_nuc1, 
        nuc2=params_nuc2, 
        prot=params_prot,
        clustering_method=args.clustering_method,
        clustering_distance=args.clustering_distance,
    )


def get_protein_representative(
    mmseqs: MMSeqs,
    input_path: pathlib.Path,
    output_prefix: pathlib.Path,
    fasta_path: pathlib.Path,
) -> None:
    db = Database(mmseqs, input_path.with_suffix(".db"))
    result = Clustering(mmseqs, output_prefix.with_suffix(".db"), db)
    subdb = result.to_subdb(output_prefix.with_name(f"{output_prefix.name}_rep_seq.db"))
    subdb.to_fasta(fasta_path)


def main(argv: typing.Optional[typing.List[str]] = None) -> int:
    # build parser and get arguments
    parser = build_parser()
    if not isinstance(argcomplete, ImportError):
        argcomplete.autocomplete(parser)
    args = parser.parse_args(argv)

    start_time = datetime.datetime.now()
    start_time_str = start_time.strftime("%Y-%m-%d %H:%M:%S")

    # extract MMseqs paremeters from command line
    params = get_mmseqs_params(args)

    # use user provided workdir or create a new one in `tempfile`
    if args.workdir is None:
        tempdir = tempfile.TemporaryDirectory(prefix="igua-")
    else:
        tempdir = pathlib.Path(args.workdir)
        tempdir.mkdir(parents=True, exist_ok=True)
        tempdir = tempfile.TemporaryDirectory(prefix="igua-", dir=workdir)

    # validate individual file arguments
    individual_args = [args.cluster_tsv, args.gff_file, args.genome_fasta, args.protein_fasta]
    using_individual_files = any(individual_args)
    if using_individual_files:
        if not all(individual_args):
            parser.error(
                "Individual file mode requires ALL of: "
                "--cluster-tsv, --gff-file, --genome-fasta, --protein-fasta"
            )

        if not args.input:
            input_dict_df = {
                "genome_id": [os.path.basename(args.genome_fasta).split(".")[0]],
                "cluster_tsv": [str(args.cluster_tsv)],
                "gff_file": [str(args.gff_file)],
                "genome_fasta_file": [str(args.genome_fasta)],
                "protein_fasta_file": [str(args.protein_fasta)],
            }

            input_df = pandas.DataFrame(input_dict_df)
            temp_tsv = workdir / "metadata.tsv"
            input_df.to_csv(temp_tsv, sep="\t", index=False)
            args.input = [temp_tsv]
    elif not args.input:
        parser.error("Input files (-i/--input) are required")

    with contextlib.ExitStack() as ctx:
        # acquire temporary folder
        workdir = pathlib.Path(ctx.enter_context(tempdir))

        # prepare progress bar
        progress = ctx.enter_context(
            rich.progress.Progress(
                "",
                rich.progress.SpinnerColumn(),
                *rich.progress.Progress.get_default_columns(),
            )
        )
        progress.console.print(f"[bold blue]{'Started':>12}[/] {start_time_str}")

        # check mmseqs version (and if an mmseqs binary is available)
        mmseqs = MMSeqs(progress=progress, threads=args.jobs, tempdir=workdir)
        try:
            v = mmseqs.version()
            progress.console.print(f"[bold green]{'Using':>12}[/] [purple]MMSeqs2[/] version {v!r}")
        except RuntimeError:
            progress.console.print(f"[bold red]{'Failed':>12}[/] to locate [purple]MMSeqs2[/] binary")
            return errno.ENOENT

        # create a pipeline with the configuration from the CLI
        pipeline = ClusteringPipeline(
            params,
            mmseqs=mmseqs,
            workdir=workdir, 
            progress=progress, 
        )

        # create appropriate dataset handler
        dataset = create_dataset(progress=progress, args=args)

        # run pipeline and retrieve GCFs
        result = pipeline.run(
            dataset, 
            clustering=args.clustering,
        )

        # save GCFs
        result.gcfs.to_csv(args.output, sep="\t", index=False)
        progress.console.print(
            f"[bold green]{'Saved':>12}[/] final GCFs table to {str(args.output)!r}"
        )

        # save compositions
        if args.compositions is not None:
            gcf_representatives = result.gcfs["gcf_representative"].unique().tolist()
            representatives_compositions = anndata.AnnData(
                X=result.compositions[gcf_representatives].X,
                var=result.compositions.var,
                obs=(
                    result.gcfs.set_index("cluster_id")
                        .loc[gcf_representatives, ["gcf_id", "gcf_representative"]]
                        .set_index("gcf_id")
                ),
            )
            representatives_compositions.write(args.compositions)
            progress.console.print(
                f"[bold green]{'Saved':>12}[/] GCF compositions to {str(args.compositions)!r}"
            )

        # save protein features
        if args.features is not None:
            get_protein_representative(
                mmseqs,
                result.proteins_faa,
                workdir.joinpath("step3"),
                args.features,
            )
            progress.console.print(
                f"[bold green]{'Saved':>12}[/] protein features to {str(args.features)!r}"
            )

    end_time = datetime.datetime.now()
    end_time_str = end_time.strftime("%Y-%m-%d %H:%M:%S")
    total_time = end_time - start_time

    total_seconds = int(total_time.total_seconds())
    hours, remainder = divmod(total_seconds, 3600)
    minutes, seconds = divmod(remainder, 60)

    if hours > 0:
        time_str = f"{hours}h {minutes}m {seconds}s"
    elif minutes > 0:
        time_str = f"{minutes}m {seconds}s"
    else:
        time_str = f"{seconds}s"

    progress.console.print(f"[bold blue]{'Completed':>12}[/] {end_time_str}")
    progress.console.print(f"[bold green]{'Total time':>12}[/] {time_str}")

    return 0
